{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/rottentomatoes/rotten_tomatoes_movies.csv\n",
      "/kaggle/input/rottentomatoes/rotten_tomatoes_critic_reviews.csv\n",
      "/kaggle/input/glove-embeddings/glove.6B.100d.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Importing the Datasets\n",
    "\n",
    "import pandas as pd\n",
    "Movies = pd.read_csv(\"/kaggle/input/rottentomatoes/rotten_tomatoes_movies.csv\")\n",
    "Reviews = pd.read_csv(\"/kaggle/input/rottentomatoes/rotten_tomatoes_critic_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rotten_tomatoes_link',\n",
       " 'critic_name',\n",
       " 'top_critic',\n",
       " 'publisher_name',\n",
       " 'review_type',\n",
       " 'review_score',\n",
       " 'review_date',\n",
       " 'review_content']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Reviews.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rotten_tomatoes_link</th>\n",
       "      <th>movie_title</th>\n",
       "      <th>movie_info</th>\n",
       "      <th>critics_consensus</th>\n",
       "      <th>content_rating</th>\n",
       "      <th>genres</th>\n",
       "      <th>directors</th>\n",
       "      <th>authors</th>\n",
       "      <th>actors</th>\n",
       "      <th>original_release_date</th>\n",
       "      <th>...</th>\n",
       "      <th>production_company</th>\n",
       "      <th>tomatometer_status</th>\n",
       "      <th>tomatometer_rating</th>\n",
       "      <th>tomatometer_count</th>\n",
       "      <th>audience_status</th>\n",
       "      <th>audience_rating</th>\n",
       "      <th>audience_count</th>\n",
       "      <th>tomatometer_top_critics_count</th>\n",
       "      <th>tomatometer_fresh_critics_count</th>\n",
       "      <th>tomatometer_rotten_critics_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Percy Jackson &amp; the Olympians: The Lightning T...</td>\n",
       "      <td>Always trouble-prone, the life of teenager Per...</td>\n",
       "      <td>Though it may seem like just another Harry Pot...</td>\n",
       "      <td>PG</td>\n",
       "      <td>Action &amp; Adventure, Comedy, Drama, Science Fic...</td>\n",
       "      <td>Chris Columbus</td>\n",
       "      <td>Craig Titley, Chris Columbus, Rick Riordan</td>\n",
       "      <td>Logan Lerman, Brandon T. Jackson, Alexandra Da...</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>...</td>\n",
       "      <td>20th Century Fox</td>\n",
       "      <td>Rotten</td>\n",
       "      <td>49.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>Spilled</td>\n",
       "      <td>53.0</td>\n",
       "      <td>254421.0</td>\n",
       "      <td>43</td>\n",
       "      <td>73</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m/0878835</td>\n",
       "      <td>Please Give</td>\n",
       "      <td>Kate (Catherine Keener) and her husband Alex (...</td>\n",
       "      <td>Nicole Holofcener's newest might seem slight i...</td>\n",
       "      <td>R</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Nicole Holofcener</td>\n",
       "      <td>Nicole Holofcener</td>\n",
       "      <td>Catherine Keener, Amanda Peet, Oliver Platt, R...</td>\n",
       "      <td>2010-04-30</td>\n",
       "      <td>...</td>\n",
       "      <td>Sony Pictures Classics</td>\n",
       "      <td>Certified-Fresh</td>\n",
       "      <td>87.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>Upright</td>\n",
       "      <td>64.0</td>\n",
       "      <td>11574.0</td>\n",
       "      <td>44</td>\n",
       "      <td>123</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m/10</td>\n",
       "      <td>10</td>\n",
       "      <td>A successful, middle-aged Hollywood songwriter...</td>\n",
       "      <td>Blake Edwards' bawdy comedy may not score a pe...</td>\n",
       "      <td>R</td>\n",
       "      <td>Comedy, Romance</td>\n",
       "      <td>Blake Edwards</td>\n",
       "      <td>Blake Edwards</td>\n",
       "      <td>Dudley Moore, Bo Derek, Julie Andrews, Robert ...</td>\n",
       "      <td>1979-10-05</td>\n",
       "      <td>...</td>\n",
       "      <td>Waner Bros.</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>67.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Spilled</td>\n",
       "      <td>53.0</td>\n",
       "      <td>14684.0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m/1000013-12_angry_men</td>\n",
       "      <td>12 Angry Men (Twelve Angry Men)</td>\n",
       "      <td>Following the closing arguments in a murder tr...</td>\n",
       "      <td>Sidney Lumet's feature debut is a superbly wri...</td>\n",
       "      <td>NR</td>\n",
       "      <td>Classics, Drama</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>Reginald Rose</td>\n",
       "      <td>Martin Balsam, John Fiedler, Lee J. Cobb, E.G....</td>\n",
       "      <td>1957-04-13</td>\n",
       "      <td>...</td>\n",
       "      <td>Criterion Collection</td>\n",
       "      <td>Certified-Fresh</td>\n",
       "      <td>100.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>Upright</td>\n",
       "      <td>97.0</td>\n",
       "      <td>105386.0</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m/1000079-20000_leagues_under_the_sea</td>\n",
       "      <td>20,000 Leagues Under The Sea</td>\n",
       "      <td>In 1866, Professor Pierre M. Aronnax (Paul Luk...</td>\n",
       "      <td>One of Disney's finest live-action adventures,...</td>\n",
       "      <td>G</td>\n",
       "      <td>Action &amp; Adventure, Drama, Kids &amp; Family</td>\n",
       "      <td>Richard Fleischer</td>\n",
       "      <td>Earl Felton</td>\n",
       "      <td>James Mason, Kirk Douglas, Paul Lukas, Peter L...</td>\n",
       "      <td>1954-01-01</td>\n",
       "      <td>...</td>\n",
       "      <td>Disney</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>89.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Upright</td>\n",
       "      <td>74.0</td>\n",
       "      <td>68918.0</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    rotten_tomatoes_link  \\\n",
       "0                              m/0814255   \n",
       "1                              m/0878835   \n",
       "2                                   m/10   \n",
       "3                 m/1000013-12_angry_men   \n",
       "4  m/1000079-20000_leagues_under_the_sea   \n",
       "\n",
       "                                         movie_title  \\\n",
       "0  Percy Jackson & the Olympians: The Lightning T...   \n",
       "1                                        Please Give   \n",
       "2                                                 10   \n",
       "3                    12 Angry Men (Twelve Angry Men)   \n",
       "4                       20,000 Leagues Under The Sea   \n",
       "\n",
       "                                          movie_info  \\\n",
       "0  Always trouble-prone, the life of teenager Per...   \n",
       "1  Kate (Catherine Keener) and her husband Alex (...   \n",
       "2  A successful, middle-aged Hollywood songwriter...   \n",
       "3  Following the closing arguments in a murder tr...   \n",
       "4  In 1866, Professor Pierre M. Aronnax (Paul Luk...   \n",
       "\n",
       "                                   critics_consensus content_rating  \\\n",
       "0  Though it may seem like just another Harry Pot...             PG   \n",
       "1  Nicole Holofcener's newest might seem slight i...              R   \n",
       "2  Blake Edwards' bawdy comedy may not score a pe...              R   \n",
       "3  Sidney Lumet's feature debut is a superbly wri...             NR   \n",
       "4  One of Disney's finest live-action adventures,...              G   \n",
       "\n",
       "                                              genres          directors  \\\n",
       "0  Action & Adventure, Comedy, Drama, Science Fic...     Chris Columbus   \n",
       "1                                             Comedy  Nicole Holofcener   \n",
       "2                                    Comedy, Romance      Blake Edwards   \n",
       "3                                    Classics, Drama       Sidney Lumet   \n",
       "4           Action & Adventure, Drama, Kids & Family  Richard Fleischer   \n",
       "\n",
       "                                      authors  \\\n",
       "0  Craig Titley, Chris Columbus, Rick Riordan   \n",
       "1                           Nicole Holofcener   \n",
       "2                               Blake Edwards   \n",
       "3                               Reginald Rose   \n",
       "4                                 Earl Felton   \n",
       "\n",
       "                                              actors original_release_date  \\\n",
       "0  Logan Lerman, Brandon T. Jackson, Alexandra Da...            2010-02-12   \n",
       "1  Catherine Keener, Amanda Peet, Oliver Platt, R...            2010-04-30   \n",
       "2  Dudley Moore, Bo Derek, Julie Andrews, Robert ...            1979-10-05   \n",
       "3  Martin Balsam, John Fiedler, Lee J. Cobb, E.G....            1957-04-13   \n",
       "4  James Mason, Kirk Douglas, Paul Lukas, Peter L...            1954-01-01   \n",
       "\n",
       "   ...      production_company  tomatometer_status tomatometer_rating  \\\n",
       "0  ...        20th Century Fox              Rotten               49.0   \n",
       "1  ...  Sony Pictures Classics     Certified-Fresh               87.0   \n",
       "2  ...             Waner Bros.               Fresh               67.0   \n",
       "3  ...    Criterion Collection     Certified-Fresh              100.0   \n",
       "4  ...                  Disney               Fresh               89.0   \n",
       "\n",
       "  tomatometer_count  audience_status  audience_rating audience_count  \\\n",
       "0             149.0          Spilled             53.0       254421.0   \n",
       "1             142.0          Upright             64.0        11574.0   \n",
       "2              24.0          Spilled             53.0        14684.0   \n",
       "3              54.0          Upright             97.0       105386.0   \n",
       "4              27.0          Upright             74.0        68918.0   \n",
       "\n",
       "   tomatometer_top_critics_count  tomatometer_fresh_critics_count  \\\n",
       "0                             43                               73   \n",
       "1                             44                              123   \n",
       "2                              2                               16   \n",
       "3                              6                               54   \n",
       "4                              5                               24   \n",
       "\n",
       "   tomatometer_rotten_critics_count  \n",
       "0                                76  \n",
       "1                                19  \n",
       "2                                 8  \n",
       "3                                 0  \n",
       "4                                 3  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rotten_tomatoes_link</th>\n",
       "      <th>critic_name</th>\n",
       "      <th>top_critic</th>\n",
       "      <th>publisher_name</th>\n",
       "      <th>review_type</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_date</th>\n",
       "      <th>review_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Ben McEachen</td>\n",
       "      <td>False</td>\n",
       "      <td>Sunday Mail (Australia)</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>3.5/5</td>\n",
       "      <td>2010-02-09</td>\n",
       "      <td>Whether audiences will get behind The Lightnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>m/0878835</td>\n",
       "      <td>Erik Childress</td>\n",
       "      <td>False</td>\n",
       "      <td>eFilmCritic.com</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>3/4</td>\n",
       "      <td>2010-01-31</td>\n",
       "      <td>Holofcener always gives us more to chew on tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>m/10</td>\n",
       "      <td>Scott Weinberg</td>\n",
       "      <td>False</td>\n",
       "      <td>eFilmCritic.com</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>4/5</td>\n",
       "      <td>2002-07-25</td>\n",
       "      <td>Obvious but entertaining portrayal of midlife ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>m/1000013-12_angry_men</td>\n",
       "      <td>Steve Rhodes</td>\n",
       "      <td>False</td>\n",
       "      <td>Internet Reviews</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>4/4</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>In a time of bloated when special effects have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>m/1000079-20000_leagues_under_the_sea</td>\n",
       "      <td>Dragan Antulov</td>\n",
       "      <td>False</td>\n",
       "      <td>rec.arts.movies.reviews</td>\n",
       "      <td>Fresh</td>\n",
       "      <td>7/10</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>[The] embodiment of Disney at his best -- fami...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      rotten_tomatoes_link     critic_name  top_critic  \\\n",
       "3                                m/0814255    Ben McEachen       False   \n",
       "153                              m/0878835  Erik Childress       False   \n",
       "292                                   m/10  Scott Weinberg       False   \n",
       "316                 m/1000013-12_angry_men    Steve Rhodes       False   \n",
       "369  m/1000079-20000_leagues_under_the_sea  Dragan Antulov       False   \n",
       "\n",
       "              publisher_name review_type review_score review_date  \\\n",
       "3    Sunday Mail (Australia)       Fresh        3.5/5  2010-02-09   \n",
       "153          eFilmCritic.com       Fresh          3/4  2010-01-31   \n",
       "292          eFilmCritic.com       Fresh          4/5  2002-07-25   \n",
       "316         Internet Reviews       Fresh          4/4  2000-01-01   \n",
       "369  rec.arts.movies.reviews       Fresh         7/10  2000-01-01   \n",
       "\n",
       "                                        review_content  \n",
       "3    Whether audiences will get behind The Lightnin...  \n",
       "153  Holofcener always gives us more to chew on tha...  \n",
       "292  Obvious but entertaining portrayal of midlife ...  \n",
       "316  In a time of bloated when special effects have...  \n",
       "369  [The] embodiment of Disney at his best -- fami...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dropping rows with NA values\n",
    "Reviews = Reviews.dropna()\n",
    "\n",
    "# Dropping Duplicates so that there is only one entry per Movie.\n",
    "Reviews = Reviews.drop_duplicates(subset=['rotten_tomatoes_link'], keep = \"first\")\n",
    "Reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rotten_tomatoes_link</th>\n",
       "      <th>movie_title</th>\n",
       "      <th>critics_consensus</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Percy Jackson &amp; the Olympians: The Lightning T...</td>\n",
       "      <td>Though it may seem like just another Harry Pot...</td>\n",
       "      <td>3.5/5</td>\n",
       "      <td>Whether audiences will get behind The Lightnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m/0878835</td>\n",
       "      <td>Please Give</td>\n",
       "      <td>Nicole Holofcener's newest might seem slight i...</td>\n",
       "      <td>3/4</td>\n",
       "      <td>Holofcener always gives us more to chew on tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m/10</td>\n",
       "      <td>10</td>\n",
       "      <td>Blake Edwards' bawdy comedy may not score a pe...</td>\n",
       "      <td>4/5</td>\n",
       "      <td>Obvious but entertaining portrayal of midlife ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m/1000013-12_angry_men</td>\n",
       "      <td>12 Angry Men (Twelve Angry Men)</td>\n",
       "      <td>Sidney Lumet's feature debut is a superbly wri...</td>\n",
       "      <td>4/4</td>\n",
       "      <td>In a time of bloated when special effects have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m/1000079-20000_leagues_under_the_sea</td>\n",
       "      <td>20,000 Leagues Under The Sea</td>\n",
       "      <td>One of Disney's finest live-action adventures,...</td>\n",
       "      <td>7/10</td>\n",
       "      <td>[The] embodiment of Disney at his best -- fami...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    rotten_tomatoes_link  \\\n",
       "0                              m/0814255   \n",
       "1                              m/0878835   \n",
       "2                                   m/10   \n",
       "3                 m/1000013-12_angry_men   \n",
       "4  m/1000079-20000_leagues_under_the_sea   \n",
       "\n",
       "                                         movie_title  \\\n",
       "0  Percy Jackson & the Olympians: The Lightning T...   \n",
       "1                                        Please Give   \n",
       "2                                                 10   \n",
       "3                    12 Angry Men (Twelve Angry Men)   \n",
       "4                       20,000 Leagues Under The Sea   \n",
       "\n",
       "                                   critics_consensus review_score  \\\n",
       "0  Though it may seem like just another Harry Pot...        3.5/5   \n",
       "1  Nicole Holofcener's newest might seem slight i...          3/4   \n",
       "2  Blake Edwards' bawdy comedy may not score a pe...          4/5   \n",
       "3  Sidney Lumet's feature debut is a superbly wri...          4/4   \n",
       "4  One of Disney's finest live-action adventures,...         7/10   \n",
       "\n",
       "                                      review_content  \n",
       "0  Whether audiences will get behind The Lightnin...  \n",
       "1  Holofcener always gives us more to chew on tha...  \n",
       "2  Obvious but entertaining portrayal of midlife ...  \n",
       "3  In a time of bloated when special effects have...  \n",
       "4  [The] embodiment of Disney at his best -- fami...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging two Dataframes\n",
    "newDf = pd.merge(Movies, Reviews, on = \"rotten_tomatoes_link\", how = \"inner\") \n",
    "\n",
    "# Dropping columns which are not required\n",
    "newDf.drop(['movie_info','content_rating','genres','directors','authors','actors','original_release_date','tomatometer_top_critics_count','tomatometer_fresh_critics_count','tomatometer_rotten_critics_count','critic_name','top_critic','publisher_name','review_type','review_date','streaming_release_date','runtime','production_company','tomatometer_status','tomatometer_rating','tomatometer_count','audience_status','audience_rating','audience_count'],axis=1,inplace=True)\n",
    "newDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '10', '100', '20', '3', '4', '5', '6', '7', '8', '9', 'A',\n",
       "       'A-', 'B', 'B+', 'B-', 'C', 'C+', 'C-', 'D', 'D+', 'D-', 'F'],\n",
       "      dtype='<U3')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fetching unique rating scale to generalize the rating\n",
    "ratingList = np.unique(newDf.review_score).tolist()\n",
    "max_score = []\n",
    "for r in range(len(ratingList)):\n",
    "    temp = ratingList[r].split(\"/\")\n",
    "    if len(temp) > 1:\n",
    "        max_score.append(temp[1])\n",
    "    elif len(temp) == 1:\n",
    "        max_score.append(temp)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "max_score = [i for b in map(lambda x:[x] if not isinstance(x, list) else x, max_score) for i in b]\n",
    "uniq_score = np.unique(max_score)\n",
    "uniq_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalizing the all the review ratings to standard scale of 5 being highest and 0 being least.\n",
    "rating = newDf.review_score.tolist()\n",
    "genrat = []\n",
    "for r in range(len(rating)):\n",
    "    temp = rating[r].split(\"/\")\n",
    "    #print(len(temp))\n",
    "    if len(temp) > 1:\n",
    "        if temp[1] == '10':\n",
    "            score = round(float(temp[0]) / 2)\n",
    "            genrat.append(score)\n",
    "        elif temp[1] == '100':\n",
    "            score = round(float(temp[0]) / 20)\n",
    "            genrat.append(score)\n",
    "        elif temp[1] == '20':\n",
    "            score = round(float(temp[0]) / 4)\n",
    "            genrat.append(score)\n",
    "        elif temp[1] == '3':\n",
    "            score = round(float(temp[0]) * 5 / 3)\n",
    "            genrat.append(score)\n",
    "        elif temp[1] == '4':\n",
    "            score = round(float(temp[0]) * 5 / 4)\n",
    "            genrat.append(score)\n",
    "        elif temp[1] == '5':\n",
    "            score = round(float(temp[0]))\n",
    "            genrat.append(score)\n",
    "        elif temp[1] == '6':\n",
    "            score = round(float(temp[0]) * 5 / 6)\n",
    "            genrat.append(score)\n",
    "        elif temp[1] == '7':\n",
    "            score = round(float(temp[0]) * 5 / 7)\n",
    "            genrat.append(score)\n",
    "        elif temp[1] == '8':\n",
    "            score = round(float(temp[0]) * 5 / 8)\n",
    "            genrat.append(score)\n",
    "        elif temp[1] == '9':\n",
    "            score = round(float(temp[0]) * 5 / 9)\n",
    "            genrat.append(score)\n",
    "    elif len(temp) == 1:\n",
    "        if temp == 'A-' or 'A' or 'A+':\n",
    "            score = 5\n",
    "            genrat.append(score)\n",
    "        elif temp == 'B-' or 'B' or 'B+':\n",
    "            score = 4\n",
    "            genrat.append(score)\n",
    "        elif temp == 'C-' or 'C' or 'C+':\n",
    "            score = 3\n",
    "            genrat.append(score)\n",
    "        elif temp == 'D-' or 'D' or 'D+':\n",
    "            score = 2\n",
    "            genrat.append(score)\n",
    "        elif temp == 'F-' or 'F' or 'F+':\n",
    "            score = 1\n",
    "            genrat.append(score)\n",
    "        else :\n",
    "            score = temp\n",
    "            genrat.append(score)\n",
    "        \n",
    "           \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDf['Standard_rating'] = genrat\n",
    "#newDf = newDf[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Preprocessing of Data\\n\\nimport spacy\\nimport numpy as np\\n#from spacy.lang.en.stop_words import STOP_WORDS\\n#nlp = en_core_web_sm.load()\\nnlp = spacy.load(\"en_core_web_sm\",disable=[\"tagger\", \"parser\"])\\ndef preprocess(text):\\n    # TODO: Replace the next line with your own code.\\n    doc = nlp(text)\\n    token_list = []\\n    for token in doc:\\n        if token.is_stop == False and token.lemma_.isalpha() and len(token) > 3:\\n            token_list.append(token.lemma_)\\n    return(token_list)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Preprocessing of Data\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "#from spacy.lang.en.stop_words import STOP_WORDS\n",
    "#nlp = en_core_web_sm.load()\n",
    "nlp = spacy.load(\"en_core_web_sm\",disable=[\"tagger\", \"parser\"])\n",
    "def preprocess(text):\n",
    "    # TODO: Replace the next line with your own code.\n",
    "    doc = nlp(text)\n",
    "    token_list = []\n",
    "    for token in doc:\n",
    "        if token.is_stop == False and token.lemma_.isalpha() and len(token) > 3:\n",
    "            token_list.append(token.lemma_)\n",
    "    return(token_list)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'newDf[\\'PreProcessedReview\\'] = None\\nfor i in range(len(newDf)):\\n    newDf[\\'PreProcessedReview\\'][i] = \" \".join(preprocess(newDf[\\'review_content\\'][i]))'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''newDf['PreProcessedReview'] = None\n",
    "for i in range(len(newDf)):\n",
    "    newDf['PreProcessedReview'][i] = \" \".join(preprocess(newDf['review_content'][i]))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rotten_tomatoes_link</th>\n",
       "      <th>movie_title</th>\n",
       "      <th>critics_consensus</th>\n",
       "      <th>review_score</th>\n",
       "      <th>review_content</th>\n",
       "      <th>Standard_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m/0814255</td>\n",
       "      <td>Percy Jackson &amp; the Olympians: The Lightning T...</td>\n",
       "      <td>Though it may seem like just another Harry Pot...</td>\n",
       "      <td>3.5/5</td>\n",
       "      <td>Whether audiences will get behind The Lightnin...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>m/0878835</td>\n",
       "      <td>Please Give</td>\n",
       "      <td>Nicole Holofcener's newest might seem slight i...</td>\n",
       "      <td>3/4</td>\n",
       "      <td>Holofcener always gives us more to chew on tha...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>m/10</td>\n",
       "      <td>10</td>\n",
       "      <td>Blake Edwards' bawdy comedy may not score a pe...</td>\n",
       "      <td>4/5</td>\n",
       "      <td>Obvious but entertaining portrayal of midlife ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>m/1000013-12_angry_men</td>\n",
       "      <td>12 Angry Men (Twelve Angry Men)</td>\n",
       "      <td>Sidney Lumet's feature debut is a superbly wri...</td>\n",
       "      <td>4/4</td>\n",
       "      <td>In a time of bloated when special effects have...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>m/1000079-20000_leagues_under_the_sea</td>\n",
       "      <td>20,000 Leagues Under The Sea</td>\n",
       "      <td>One of Disney's finest live-action adventures,...</td>\n",
       "      <td>7/10</td>\n",
       "      <td>[The] embodiment of Disney at his best -- fami...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    rotten_tomatoes_link  \\\n",
       "0                              m/0814255   \n",
       "1                              m/0878835   \n",
       "2                                   m/10   \n",
       "3                 m/1000013-12_angry_men   \n",
       "4  m/1000079-20000_leagues_under_the_sea   \n",
       "\n",
       "                                         movie_title  \\\n",
       "0  Percy Jackson & the Olympians: The Lightning T...   \n",
       "1                                        Please Give   \n",
       "2                                                 10   \n",
       "3                    12 Angry Men (Twelve Angry Men)   \n",
       "4                       20,000 Leagues Under The Sea   \n",
       "\n",
       "                                   critics_consensus review_score  \\\n",
       "0  Though it may seem like just another Harry Pot...        3.5/5   \n",
       "1  Nicole Holofcener's newest might seem slight i...          3/4   \n",
       "2  Blake Edwards' bawdy comedy may not score a pe...          4/5   \n",
       "3  Sidney Lumet's feature debut is a superbly wri...          4/4   \n",
       "4  One of Disney's finest live-action adventures,...         7/10   \n",
       "\n",
       "                                      review_content  Standard_rating  \n",
       "0  Whether audiences will get behind The Lightnin...                4  \n",
       "1  Holofcener always gives us more to chew on tha...                4  \n",
       "2  Obvious but entertaining portrayal of midlife ...                4  \n",
       "3  In a time of bloated when special effects have...                5  \n",
       "4  [The] embodiment of Disney at his best -- fami...                4  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balancing Dataset by removing the Movies which has rating 0 and 1 \n",
    "# since the number of movies available for each of the rating is very less\n",
    "newDf = newDf[newDf.Standard_rating != 0]\n",
    "newDf = newDf[newDf.Standard_rating != 1]\n",
    "newDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing the Dataset such that there are equal(almost equal) number of movies for each rating\n",
    "g = newDf.groupby('Standard_rating')\n",
    "bal_data = g.apply(lambda x: x.sample(g.size().min()).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_rating(rating):\n",
    "    rating = int(rating)\n",
    "    if rating == 2:\n",
    "        return 0\n",
    "    elif rating == 3:\n",
    "        return 1\n",
    "    elif rating == 4:\n",
    "        return 2\n",
    "    else: \n",
    "        return 3\n",
    "\n",
    "# Reassigning the ratings of movies from scale 2-5 to 0-3 because for BERT outputs softmax probability for each rating\n",
    "# and in order to pick the maximum probablity index. \n",
    "bal_data['Standard_rating'] = bal_data.Standard_rating.apply(std_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reviews = bal_data.PreProcessedReview.values\n",
    "reviews = bal_data.review_content.values\n",
    "labels = bal_data.Standard_rating.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_sentences, validation_sentences, train_labels, validation_labels = train_test_split(reviews, labels, \n",
    "                                                                                          test_size=0.2, \n",
    "                                                                                          stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.49      0.47       613\n",
      "           1       0.35      0.34      0.35       612\n",
      "           2       0.41      0.40      0.40       613\n",
      "           3       0.35      0.33      0.34       613\n",
      "\n",
      "    accuracy                           0.39      2451\n",
      "   macro avg       0.39      0.39      0.39      2451\n",
      "weighted avg       0.39      0.39      0.39      2451\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "pipe_LR = Pipeline([('countvectorizer', vectorizer),('LR',LogisticRegression())])\n",
    "pipe_LR.fit(train_sentences, train_labels)\n",
    "pred_LR = pipe_LR.predict(validation_sentences)\n",
    "print(classification_report(validation_labels, pred_LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.53      0.45       613\n",
      "           1       0.37      0.37      0.37       612\n",
      "           2       0.36      0.42      0.39       613\n",
      "           3       0.36      0.17      0.24       613\n",
      "\n",
      "    accuracy                           0.37      2451\n",
      "   macro avg       0.37      0.37      0.36      2451\n",
      "weighted avg       0.37      0.37      0.36      2451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "pipe_SVC = Pipeline([('countvectorizer', vectorizer),('SVC',SVC())])\n",
    "pipe_SVC.fit(train_sentences, train_labels)\n",
    "pred_SVC = pipe_SVC.predict(validation_sentences)\n",
    "print(classification_report(validation_labels, pred_SVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.43      0.36       613\n",
      "           1       0.33      0.25      0.28       612\n",
      "           2       0.30      0.36      0.32       613\n",
      "           3       0.26      0.18      0.21       613\n",
      "\n",
      "    accuracy                           0.30      2451\n",
      "   macro avg       0.30      0.30      0.30      2451\n",
      "weighted avg       0.30      0.30      0.30      2451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "pipe_DT = Pipeline([('countvectorizer', vectorizer),('decisiontree',DecisionTreeClassifier(max_depth=50))])\n",
    "pipe_DT.fit(train_sentences, train_labels)\n",
    "pred_DT = pipe_DT.predict(validation_sentences)\n",
    "print(classification_report(validation_labels, pred_DT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4c8ba92910>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot of length of reviews\n",
    "\n",
    "senlen = [len(train_sentences[i]) for i in range(len(train_sentences))]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(len(senlen)),senlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import pandas as pd\n",
    "import torch\n",
    "#from pytorch_pretrained_bert import BertModel\n",
    "from torch import nn\n",
    "#from pytorch_pretrained_bert import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (3.5.1)\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.1)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.7/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.45.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.10)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.4.4)\n",
      "Requirement already satisfied: tokenizers==0.9.3 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.9.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (1.14.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.45.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.1; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ed6cb609fc4ee8bc2e0c60529c5066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descriptiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing inputs and adding special tokens\n",
    "\n",
    "def input_id(input_review):\n",
    "    input_ids = []\n",
    "    MAX_LEN = 256\n",
    "    for r in input_review:\n",
    "        encoded_sent = tokenizer.encode(r,add_special_tokens = True) # Special tokens '[CLS]' and '[SEP]'\n",
    "        input_ids.append(encoded_sent)\n",
    "    return input_ids\n",
    "train_id = input_id(train_sentences)\n",
    "val_id = input_id(validation_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the Tokenized IDs\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "padded_train_id = pad_sequences(train_id, maxlen=256 , truncating=\"post\", padding=\"post\")\n",
    "padded_val_id = pad_sequences(val_id, maxlen=256 , truncating=\"post\", padding=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking the padded ID's\n",
    "\n",
    "def masking(pid):\n",
    "    attention_masks = []\n",
    "\n",
    "    for r in pid:\n",
    "    \n",
    "        # Generating attention mask for sentences.\n",
    "        #   - when there is 0 present as token id we are going to set mask as 0.\n",
    "        #   - we are going to set mask 1 for all non-zero positive input id.\n",
    "        att_mask = [int(token_id > 0) for token_id in r]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    return attention_masks\n",
    "\n",
    "train_mask = masking(padded_train_id)\n",
    "val_mask = masking(padded_val_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the numpy arrays into tensors for working on GPU.\n",
    "import torch\n",
    "\n",
    "tr_inputs = torch.tensor(padded_train_id)\n",
    "val_inputs = torch.tensor(padded_val_id)\n",
    "\n",
    "tr_labels = torch.tensor(train_labels.reshape(-1, 1)).float()\n",
    "val_labels = torch.tensor(validation_labels.reshape(-1, 1)).float()\n",
    "\n",
    "tr_masks = torch.tensor(train_mask)\n",
    "val_masks = torch.tensor(val_mask)\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Deciding the batch size for training.\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "#DataLoader for our training set.\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# DataLoader for our validation(test) set.\n",
    "validation_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building custom BERT model with additional Classification layer\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes = 4):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(768, n_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "  \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \n",
    "        _, pooled_output = self.bert(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "        output = self.drop(pooled_output)\n",
    "        #print(torch.max(output,dim = 1))\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for GPU processors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "torch.cuda.empty_cache()     # Clearing Cache space for fresh Model run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8dbb607ede433ca2124e20cade0ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070fe81025494fd8b8f6d2ad88431e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Assigning model to GPU\n",
    "\n",
    "model = BertClassifier()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=3e-6)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5\n",
      "\r",
      "1225/1225.125 loss: 0.9603278455833432 \n"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "\n",
    "for epoch_num in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for step_num, batch_data in enumerate(train_dataloader):\n",
    "        token_ids, masks, targets = tuple(t.to(device).long() for t in batch_data)\n",
    "\n",
    "        outputs = model(\n",
    "      token_ids,\n",
    "      masks\n",
    "    )\n",
    "        #print(len(targets))\n",
    "        targets = targets.reshape((1,len(targets)))\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets[0])\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "\n",
    "        clip_grad_norm_(parameters=model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print('Epoch: ', epoch_num + 1)\n",
    "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / batch_size, train_loss / (step_num + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating and Predicting the Movie Ratings for Validation data\n",
    "\n",
    "model.eval()\n",
    "bert_predicted = []\n",
    "with torch.no_grad():\n",
    "    for step_num, batch_data in enumerate(validation_dataloader):\n",
    "        token_ids, masks, targets = tuple(t.to(device).long() for t in batch_data)\n",
    "\n",
    "        outputs = model(\n",
    "      token_ids,\n",
    "      masks\n",
    "    )\n",
    "        \n",
    "        targets = targets.reshape((1,len(targets)))\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        bert_predicted.append(preds)\n",
    "    \n",
    "pred = []\n",
    "for i in range(len(bert_predicted)):\n",
    "    for j in range(len(bert_predicted[i])):\n",
    "        pred.append(bert_predicted[i][j])\n",
    "    \n",
    "pred = torch.tensor(pred).numpy()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.60      0.59       613\n",
      "         1.0       0.43      0.32      0.37       612\n",
      "         2.0       0.43      0.67      0.52       613\n",
      "         3.0       0.44      0.28      0.34       613\n",
      "\n",
      "    accuracy                           0.47      2451\n",
      "   macro avg       0.47      0.47      0.45      2451\n",
      "weighted avg       0.47      0.47      0.45      2451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_labels, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below code is used only for miscellaneous comparison of accuracies with Word Emedding, LSTM, GRU and Glove Embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.get_dummies(bal_data.Standard_rating.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_sentences, validation_sentences, train_labels, validation_labels = train_test_split(reviews, labels, \n",
    "                                                                                          test_size=0.2, \n",
    "                                                                                          stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentences\n",
    "tokenizer = Tokenizer(num_words = 1500)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# convert train dataset to sequence and pad sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_sequences, padding='post', maxlen=120)\n",
    "\n",
    "# convert validation dataset to sequence and pad sequences\n",
    "validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\n",
    "validation_padded = pad_sequences(validation_sequences, padding='post', maxlen=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 120, 100)          2135500   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                2424      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 100       \n",
      "=================================================================\n",
      "Total params: 2,138,024\n",
      "Trainable params: 2,138,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Word Embedding\n",
    "\n",
    "# model initialization\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(word_index), output_dim= 100, input_length=120),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "245/245 - 5s - loss: 0.5824 - accuracy: 0.2514 - val_loss: 0.5625 - val_accuracy: 0.2407\n",
      "Epoch 2/10\n",
      "245/245 - 5s - loss: 0.5618 - accuracy: 0.2682 - val_loss: 0.5618 - val_accuracy: 0.2611\n",
      "Epoch 3/10\n",
      "245/245 - 5s - loss: 0.5609 - accuracy: 0.2816 - val_loss: 0.5608 - val_accuracy: 0.2800\n",
      "Epoch 4/10\n",
      "245/245 - 4s - loss: 0.5586 - accuracy: 0.3171 - val_loss: 0.5589 - val_accuracy: 0.3029\n",
      "Epoch 5/10\n",
      "245/245 - 5s - loss: 0.5536 - accuracy: 0.3413 - val_loss: 0.5544 - val_accuracy: 0.3029\n",
      "Epoch 6/10\n",
      "245/245 - 4s - loss: 0.5447 - accuracy: 0.3815 - val_loss: 0.5498 - val_accuracy: 0.3406\n",
      "Epoch 7/10\n",
      "245/245 - 4s - loss: 0.5337 - accuracy: 0.4092 - val_loss: 0.5436 - val_accuracy: 0.3580\n",
      "Epoch 8/10\n",
      "245/245 - 5s - loss: 0.5210 - accuracy: 0.4287 - val_loss: 0.5383 - val_accuracy: 0.3692\n",
      "Epoch 9/10\n",
      "245/245 - 4s - loss: 0.5103 - accuracy: 0.4513 - val_loss: 0.5368 - val_accuracy: 0.3733\n",
      "Epoch 10/10\n",
      "245/245 - 5s - loss: 0.5021 - accuracy: 0.4556 - val_loss: 0.5376 - val_accuracy: 0.3672\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.24      0.31       612\n",
      "           1       0.31      0.31      0.31       613\n",
      "           2       0.36      0.46      0.40       613\n",
      "           3       0.31      0.35      0.33       613\n",
      "\n",
      "    accuracy                           0.34      2451\n",
      "   macro avg       0.35      0.34      0.34      2451\n",
      "weighted avg       0.35      0.34      0.34      2451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "val_labels = np.array(validation_labels.idxmax(axis=1))\n",
    "num_epochs = 10\n",
    "history = model.fit(train_padded, train_labels, \n",
    "                    epochs=num_epochs, verbose=2, \n",
    "                    validation_split=0.2)\n",
    "\n",
    "# predict values\n",
    "pred = model.predict(validation_padded)\n",
    "ind_pred = []\n",
    "for i in range(len(pred)):\n",
    "    ind_pred.append(np.where(pred[i] == max(pred[i]))[0])\n",
    "print(classification_report(val_labels, ind_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 120, 100)          2135500   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 120, 128)          84480     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                41216     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                1560      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 100       \n",
      "=================================================================\n",
      "Total params: 2,262,856\n",
      "Trainable params: 2,262,856\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "model1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(word_index), output_dim= 100, input_length=120),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model1.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model summary\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "245/245 - 11s - loss: 0.5698 - accuracy: 0.2532 - val_loss: 0.5653 - val_accuracy: 0.2458\n",
      "Epoch 2/10\n",
      "245/245 - 10s - loss: 0.5598 - accuracy: 0.2851 - val_loss: 0.5554 - val_accuracy: 0.3259\n",
      "Epoch 3/10\n",
      "245/245 - 10s - loss: 0.5319 - accuracy: 0.3866 - val_loss: 0.5437 - val_accuracy: 0.3621\n",
      "Epoch 4/10\n",
      "245/245 - 10s - loss: 0.5005 - accuracy: 0.4570 - val_loss: 0.5499 - val_accuracy: 0.3682\n",
      "Epoch 5/10\n",
      "245/245 - 10s - loss: 0.4709 - accuracy: 0.5224 - val_loss: 0.5677 - val_accuracy: 0.3692\n",
      "Epoch 6/10\n",
      "245/245 - 9s - loss: 0.4455 - accuracy: 0.5656 - val_loss: 0.5810 - val_accuracy: 0.3590\n",
      "Epoch 7/10\n",
      "245/245 - 10s - loss: 0.4183 - accuracy: 0.6121 - val_loss: 0.6006 - val_accuracy: 0.3575\n",
      "Epoch 8/10\n",
      "245/245 - 10s - loss: 0.3909 - accuracy: 0.6533 - val_loss: 0.6467 - val_accuracy: 0.3493\n",
      "Epoch 9/10\n",
      "245/245 - 9s - loss: 0.3683 - accuracy: 0.6832 - val_loss: 0.6555 - val_accuracy: 0.3519\n",
      "Epoch 10/10\n",
      "245/245 - 10s - loss: 0.3455 - accuracy: 0.7079 - val_loss: 0.6683 - val_accuracy: 0.3391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.39      0.39       612\n",
      "           1       0.30      0.35      0.32       613\n",
      "           2       0.32      0.27      0.29       613\n",
      "           3       0.31      0.30      0.31       613\n",
      "\n",
      "    accuracy                           0.33      2451\n",
      "   macro avg       0.33      0.33      0.33      2451\n",
      "weighted avg       0.33      0.33      0.33      2451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "num_epochs = 10\n",
    "history1 = model1.fit(train_padded, train_labels, \n",
    "                    epochs=num_epochs, verbose=2, \n",
    "                    validation_split=0.2)\n",
    "\n",
    "# predict values\n",
    "pred1 = model1.predict(validation_padded)\n",
    "ind_pred = []\n",
    "for i in range(len(pred)):\n",
    "    ind_pred.append(np.where(pred1[i] == max(pred1[i]))[0])\n",
    "print(classification_report(val_labels, ind_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 120, 100)          2135500   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 128)               63744     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 24)                3096      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 100       \n",
      "=================================================================\n",
      "Total params: 2,202,440\n",
      "Trainable params: 2,202,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# GRU\n",
    "\n",
    "model3 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(word_index), output_dim= 100, input_length=120),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64)),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model3.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model summary\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "245/245 - 7s - loss: 0.5641 - accuracy: 0.2746 - val_loss: 0.5464 - val_accuracy: 0.3218\n",
      "Epoch 2/10\n",
      "245/245 - 7s - loss: 0.5322 - accuracy: 0.3684 - val_loss: 0.5423 - val_accuracy: 0.3457\n",
      "Epoch 3/10\n",
      "245/245 - 7s - loss: 0.5030 - accuracy: 0.4454 - val_loss: 0.5489 - val_accuracy: 0.3524\n",
      "Epoch 4/10\n",
      "245/245 - 7s - loss: 0.4706 - accuracy: 0.5159 - val_loss: 0.5648 - val_accuracy: 0.3590\n",
      "Epoch 5/10\n",
      "245/245 - 8s - loss: 0.4398 - accuracy: 0.5598 - val_loss: 0.6075 - val_accuracy: 0.3549\n",
      "Epoch 6/10\n",
      "245/245 - 7s - loss: 0.4129 - accuracy: 0.6082 - val_loss: 0.6125 - val_accuracy: 0.3457\n",
      "Epoch 7/10\n",
      "245/245 - 7s - loss: 0.3908 - accuracy: 0.6426 - val_loss: 0.6459 - val_accuracy: 0.3508\n",
      "Epoch 8/10\n",
      "245/245 - 7s - loss: 0.3642 - accuracy: 0.6731 - val_loss: 0.6727 - val_accuracy: 0.3514\n",
      "Epoch 9/10\n",
      "245/245 - 7s - loss: 0.3379 - accuracy: 0.7103 - val_loss: 0.7610 - val_accuracy: 0.3452\n",
      "Epoch 10/10\n",
      "245/245 - 7s - loss: 0.3113 - accuracy: 0.7335 - val_loss: 0.7539 - val_accuracy: 0.3355\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.37      0.38       612\n",
      "           1       0.30      0.34      0.32       613\n",
      "           2       0.32      0.29      0.31       613\n",
      "           3       0.31      0.32      0.32       613\n",
      "\n",
      "    accuracy                           0.33      2451\n",
      "   macro avg       0.33      0.33      0.33      2451\n",
      "weighted avg       0.33      0.33      0.33      2451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "num_epochs = 10\n",
    "history3 = model3.fit(train_padded, train_labels, \n",
    "                    epochs=num_epochs, verbose=2, \n",
    "                    validation_split=0.2)\n",
    "\n",
    "# predict values\n",
    "pred3 = model3.predict(validation_padded)\n",
    "ind_pred = []\n",
    "for i in range(len(pred)):\n",
    "    ind_pred.append(np.where(pred3[i] == max(pred3[i]))[0])\n",
    "print(classification_report(val_labels, ind_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open('/kaggle/input/glove-embeddings/glove.6B.100d.txt') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embeddings_matrix = np.zeros((len(word_index) + 1, 100))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 120, 100)          2135600   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 24)                2424      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 100       \n",
      "=================================================================\n",
      "Total params: 2,138,124\n",
      "Trainable params: 2,524\n",
      "Non-trainable params: 2,135,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model initialization\n",
    "model4 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(word_index)+1, output_dim= 100, input_length=120, weights=[embeddings_matrix], trainable=False),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model4.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model summary\n",
    "model4.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "245/245 - 1s - loss: 0.6021 - accuracy: 0.2587 - val_loss: 0.5715 - val_accuracy: 0.2453\n",
      "Epoch 2/10\n",
      "245/245 - 1s - loss: 0.5666 - accuracy: 0.2745 - val_loss: 0.5636 - val_accuracy: 0.2789\n",
      "Epoch 3/10\n",
      "245/245 - 1s - loss: 0.5608 - accuracy: 0.2909 - val_loss: 0.5604 - val_accuracy: 0.2963\n",
      "Epoch 4/10\n",
      "245/245 - 1s - loss: 0.5580 - accuracy: 0.3017 - val_loss: 0.5588 - val_accuracy: 0.2815\n",
      "Epoch 5/10\n",
      "245/245 - 1s - loss: 0.5564 - accuracy: 0.3089 - val_loss: 0.5576 - val_accuracy: 0.3090\n",
      "Epoch 6/10\n",
      "245/245 - 1s - loss: 0.5549 - accuracy: 0.3186 - val_loss: 0.5566 - val_accuracy: 0.3055\n",
      "Epoch 7/10\n",
      "245/245 - 1s - loss: 0.5535 - accuracy: 0.3191 - val_loss: 0.5552 - val_accuracy: 0.3197\n",
      "Epoch 8/10\n",
      "245/245 - 1s - loss: 0.5524 - accuracy: 0.3286 - val_loss: 0.5542 - val_accuracy: 0.3182\n",
      "Epoch 9/10\n",
      "245/245 - 1s - loss: 0.5509 - accuracy: 0.3378 - val_loss: 0.5534 - val_accuracy: 0.3228\n",
      "Epoch 10/10\n",
      "245/245 - 1s - loss: 0.5497 - accuracy: 0.3397 - val_loss: 0.5527 - val_accuracy: 0.3253\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.49      0.42       612\n",
      "           1       0.29      0.17      0.21       613\n",
      "           2       0.41      0.15      0.22       613\n",
      "           3       0.31      0.53      0.39       613\n",
      "\n",
      "    accuracy                           0.34      2451\n",
      "   macro avg       0.34      0.34      0.31      2451\n",
      "weighted avg       0.34      0.34      0.31      2451\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "num_epochs = 10\n",
    "history4 = model4.fit(train_padded, train_labels, \n",
    "                    epochs=num_epochs, verbose=2, \n",
    "                    validation_split=0.2)\n",
    "\n",
    "# predict values\n",
    "pred4 = model4.predict(validation_padded)\n",
    "ind_pred = []\n",
    "for i in range(len(pred)):\n",
    "    ind_pred.append(np.where(pred4[i] == max(pred4[i]))[0])\n",
    "print(classification_report(val_labels, ind_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
